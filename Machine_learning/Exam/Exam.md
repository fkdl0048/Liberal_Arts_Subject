# 기말고사 대비

## 머신러닝 소개

### 머신러닝의 정의

머신러닝은 데이터를 이용하여 자동으로 학습하고, 이를 통해 예측하거나 결정을 내리는 알고리즘을 만드는 과정이다.
인간의 명시적인 프로그래밍 없이 컴퓨터가 경험을 통해 학습한다.

### 머신러닝의 필요성

대량의 데이터를 처리하여 유의미한 정보를 도출할 수 있다.
복잡한 패턴을 인식하고, 예측을 통해 비즈니스 인사이트를 제공한다.
자동화된 의사결정 시스템을 구현하여 효율성을 높인다.

### 머신러닝의 기본 개념

- 데이터: 머신러닝 모델의 학습을 위한 기초 자료로, 입력 데이터와 목표 데이터로 구분된다.
알고리즘: 데이터를 학습하여 예측 모델을 생성하는 절차와 규칙 집합.
- 모델: 학습된 알고리즘의 결과로, 새로운 데이터에 대한 예측을 수행한다.
머신러닝의 종류
- 지도 학습 (Supervised Learning): 입력과 출력이 짝지어진 데이터를 이용하여 학습. 예: 회귀, 분류.
- 비지도 학습 (Unsupervised Learning): 출력 없이 입력 데이터의 패턴을 찾는 학습. 예: 클러스터링.
- 강화 학습 (Reinforcement Learning): 행위와 보상을 통해 학습하는 방법. 예: 게임 인공지능.

### 머신러닝의 응용 분야

- 이미지 인식: 얼굴 인식, 객체 검출 등.
- 자연어 처리: 번역, 감정 분석 등.
- 예측 분석: 판매 예측, 위험 관리 등.

### 머신러닝 프로세스

- 데이터 수집: 다양한 출처에서 데이터를 수집.
- 데이터 전처리: 결측치 처리, 데이터 정규화 등.
- 모델 학습: 데이터를 이용하여 모델을 학습.
- 모델 평가: 모델의 성능을 평가하고, 개선 필요 시 재학습.

## 지도 학습 (Supervised Learning)

### 지도 학습의 정의

지도 학습은 입력 데이터와 해당 출력 데이터(라벨)를 이용하여 모델을 학습시키는 방법이다.
주어진 입력에 대해 올바른 출력을 예측하도록 모델을 훈련시킨다.
지도 학습의 필요성

데이터에 라벨이 있는 경우, 라벨을 예측하는 데 사용된다.
예: 이메일 분류(스팸/비스팸), 이미지 인식(개/고양이)

### 지도 학습의 과정

- 데이터 수집: 입력과 출력이 짝지어진 데이터를 수집.
- 데이터 전처리: 결측치 처리, 데이터 정규화 등.
- 모델 선택: 예측을 수행할 수 있는 적절한 알고리즘 선택.
- 모델 학습: 데이터를 이용하여 모델을 학습.
- 모델 평가: 테스트 데이터를 사용하여 모델의 성능 평가.
- 주요 알고리즘

### 회귀(Regression): 연속적인 값을 예측하는 문제에 사용.

- 예: 주택 가격 예측
  - 주요 알고리즘: 선형 회귀(Linear Regression), 다중 선형 회귀(Multiple Linear Regression)
  - 분류(Classification): 이산적인 클래스를 예측하는 문제에 사용.
- 예: 이메일 스팸 필터링
  - 주요 알고리즘: 로지스틱 회귀(Logistic Regression), 서포트 벡터 머신(Support Vector Machine), 결정 트리(Decision Tree), 랜덤 포레스트(Random Forest), k-최근접 이웃(k-Nearest Neighbors)

### 모델 평가 방법

- 교차 검증(Cross-Validation): 데이터를 여러 번 분할하여 모델을 학습 및 평가.
- 정확도(Accuracy): 전체 예측 중 올바른 예측의 비율.
- **정밀도(Precision)**와 재현율(Recall): 분류 문제에서의 성능 평가 지표.
- F1 스코어(F1 Score): 정밀도와 재현율의 조화 평균.

### 과대적합과 과소적합

과대적합(Overfitting): 모델이 학습 데이터에 너무 잘 맞아 새로운 데이터에 대한 일반화 성능이 떨어짐.
과소적합(Underfitting): 모델이 학습 데이터의 패턴을 충분히 학습하지 못함.
해결 방법: 교차 검증, 정규화 기법 사용, 적절한 모델 복잡도 선택.

## 비지도 학습 (Unsupervised Learning)

### 비지도 학습의 정의

비지도 학습은 데이터에 라벨이 없는 경우, 데이터의 구조를 학습하는 방법이다.
데이터의 숨겨진 패턴이나 군집을 찾아내는 것이 목표이다.

### 비지도 학습의 필요성

많은 데이터는 라벨이 없는 경우가 많다.
라벨링되지 않은 데이터를 활용하여 유용한 정보를 추출할 수 있다.
예: 고객 세분화, 이미지 군집화

### 비지도 학습의 과정

- 데이터 수집: 라벨이 없는 데이터를 수집.
- 데이터 전처리: 결측치 처리, 데이터 정규화 등.
- 모델 선택: 데이터의 구조를 학습할 수 있는 적절한 알고리즘 선택.
- 모델 학습: 데이터를 이용하여 모델을 학습.
- 모델 평가: 학습된 모델의 성능 평가.

### 주요 알고리즘

- 군집화 (Clustering): 데이터 포인트들을 군집으로 묶는 방법.
- 주요 알고리즘: k-평균 군집화(k-Means Clustering), 계층적 군집화(Hierarchical Clustering), DBSCAN
- 차원 축소 (Dimensionality Reduction): 고차원의 데이터를 저차원으로 변환.
- 주요 알고리즘: 주성분 분석(Principal Component Analysis, PCA), t-SNE
- 연관 규칙 학습 (Association Rule Learning): 데이터 간의 연관성을 발견.
- 주요 알고리즘: Apriori, Eclat

### 모델 평가 방법

- 엘보우 방법(Elbow Method): 군집의 수를 정할 때 사용.
- 실루엣 분석(Silhouette Analysis): 군집화의 품질 평가.
- 재구성 오류(Reconstruction Error): 차원 축소 모델의 성능 평가.

### 비지도 학습의 응용 분야

- 고객 세분화: 마케팅 전략 수립을 위해 고객을 그룹화.
- 이미지 군집화: 유사한 이미지를 그룹화하여 관리.
- 이상 탐지(Anomaly Detection): 정상 패턴에서 벗어난 데이터 탐지.

## 모델 평가 및 선택 (Model Evaluation and Selection)

### 모델 평가의 중요성

모델 평가란 학습된 모델의 성능을 측정하고, 이를 기반으로 모델의 적합성을 판단하는 과정이다. 모델이 얼마나 잘 예측하는지를 평가하는 것은 모델 선택 및 개선에 있어서 매우 중요하다.

### 모델 평가 방법

1. **훈련 세트와 테스트 세트 분할**
   - 데이터를 훈련 세트와 테스트 세트로 나누어 훈련 세트로 모델을 학습시키고, 테스트 세트로 모델의 성능을 평가한다.

2. **교차 검증 (Cross-Validation)**
   - 데이터를 여러 번 분할하여 학습 및 평가를 반복하는 방법.
   - **k-겹 교차 검증**: 데이터를 k개의 부분으로 나누어 각각의 부분을 테스트 세트로 사용하고, 나머지를 훈련 세트로 사용하여 k번 반복 평가.

3. **평가 지표**
   - **정확도 (Accuracy)**: 전체 예측 중 올바른 예측의 비율.
   - **정밀도 (Precision)**: 양성 예측 중 실제 양성의 비율.
   - **재현율 (Recall)**: 실제 양성 중 올바르게 예측된 비율.
   - **F1 스코어**: 정밀도와 재현율의 조화 평균.
   - **ROC 곡선 및 AUC (Area Under the Curve)**: 이진 분류 모델의 성능을 시각화하고 평가.

### 모델 선택

1. **과적합과 과소적합**
   - **과적합 (Overfitting)**: 모델이 학습 데이터에 너무 잘 맞아 새로운 데이터에 대한 일반화 성능이 떨어짐.
   - **과소적합 (Underfitting)**: 모델이 학습 데이터의 패턴을 충분히 학습하지 못함.
   - 해결 방법: 교차 검증, 정규화 기법 사용, 적절한 모델 복잡도 선택.

2. **하이퍼파라미터 튜닝**
   - 모델의 성능을 최적화하기 위해 하이퍼파라미터를 조정하는 과정.
   - **그리드 서치 (Grid Search)**: 하이퍼파라미터의 가능한 모든 조합을 시도하여 최적의 조합을 찾는 방법.
   - **랜덤 서치 (Random Search)**: 하이퍼파라미터 공간에서 무작위로 조합을 시도하는 방법.

3. **모델의 앙상블**
   - 여러 모델을 결합하여 더 나은 성능을 도출하는 방법.
   - **배깅 (Bagging)**: 여러 모델을 독립적으로 학습시킨 후, 그 예측을 결합하여 최종 예측을 수행.
   - **부스팅 (Boosting)**: 모델을 순차적으로 학습시켜 이전 모델의 오류를 개선.

### 실제 사례

1. **자동차 가격 예측**
   - 다양한 특성을 가진 자동차 데이터를 사용하여 가격을 예측하는 회귀 모델을 학습.
   - 평가 지표: 평균 절대 오차 (MAE), 평균 제곱 오차 (MSE) 등.

2. **코로나19 테스트 결과 분류**
   - 환자의 체온 및 기타 증상 데이터를 사용하여 코로나19 테스트 결과를 예측하는 이진 분류 모델을 학습.
   - 평가 지표: 정확도, 정밀도, 재현율, F1 스코어 등.

## 분류 (Classification)

### 분류의 정의

분류는 데이터 포인트를 미리 정의된 클래스 레이블 중 하나로 분류하는 문제이다. 주로 이산적인 값을 예측하는 데 사용된다.

### 분류의 필요성

분류는 다양한 응용 분야에서 중요한 역할을 한다. 예를 들어, 스팸 이메일 필터링, 질병 진단, 이미지 인식 등에서 사용된다.

### 분류의 과정

1. **데이터 수집**: 입력 데이터와 해당 클래스 레이블을 수집.
2. **데이터 전처리**: 결측치 처리, 데이터 정규화 등.
3. **모델 선택**: 데이터를 기반으로 예측을 수행할 수 있는 적절한 알고리즘 선택.
4. **모델 학습**: 데이터를 이용하여 모델을 학습.
5. **모델 평가**: 학습된 모델의 성능을 평가.

### 주요 알고리즘

- **로지스틱 회귀 (Logistic Regression)**: 이진 분류 문제에서 주로 사용.
- **서포트 벡터 머신 (Support Vector Machine)**: 클래스 간의 마진을 최대화하는 결정 경계를 찾음.
- **결정 트리 (Decision Tree)**: 데이터의 특성을 이용하여 분할 규칙을 학습.
- **랜덤 포레스트 (Random Forest)**: 여러 결정 트리를 결합하여 예측 성능을 향상.
- **k-최근접 이웃 (k-Nearest Neighbors)**: 데이터 포인트의 가장 가까운 k개의 이웃을 기반으로 분류.

### 모델 평가 방법

- **정확도 (Accuracy)**: 전체 예측 중 올바른 예측의 비율.
- **정밀도 (Precision)**와 **재현율 (Recall)**: 분류 문제에서의 성능 평가 지표.
- **F1 스코어 (F1 Score)**: 정밀도와 재현율의 조화 평균.
- **ROC 곡선 및 AUC (Area Under the Curve)**: 이진 분류 모델의 성능을 시각화하고 평가.

### 실제 사례

1. **손글씨 숫자 인식**
   - 손글씨로 작성된 숫자를 자동으로 인식하는 문제.
   - 입력: 28×28 픽셀 이미지.
   - 출력: 0부터 9까지의 숫자 레이블.

2. **코로나19 테스트 결과 분류**
   - 환자의 체온 및 기타 증상 데이터를 사용하여 코로나19 테스트 결과를 예측하는 이진 분류 모델을 학습.
   - 평가 지표: 정확도, 정밀도, 재현율, F1 스코어 등.

3. **이미지 분류**
   - 이미지의 특징을 기반으로 예측 함수를 적용하여 원하는 클래스 레이블을 얻음.
   - 예: 사과 이미지를 'apple' 클래스로 분류.

이와 같은 내용을 통해 분류의 기본 개념과 주요 알고리즘, 그리고 모델 평가 방법을 이해할 수 있습니다.

## 신경회로망 모델링 (Neural Network Modeling)

### 비선형 예측 함수 (Nonlinear Prediction Function)

비선형 예측 함수는 데이터의 복잡한 패턴을 학습하고 예측하는 데 사용된다. 이는 선형 모델이 해결할 수 없는 문제들을 해결하는 데 도움을 준다.

### 뉴런과 브레인 (Neurons and the Brain)

- **뉴런의 구조**: 뉴런은 신경계의 기본 단위로, 신호를 전달하고 처리하는 역할을 한다.
- **브레인의 작동 원리**: 여러 뉴런이 연결되어 신경 회로를 구성하며, 이를 통해 복잡한 정보 처리와 학습이 이루어진다.

### 뉴런 모델 표현 (Neuron Model Representation)

- **활성화 함수**: 뉴런의 출력 값을 결정하는 함수로, 시그모이드 함수, ReLU 등이 있다.
- **뉴런의 수학적 모델**: 입력 값들의 가중치 합을 계산하고, 이를 활성화 함수를 통해 출력 값으로 변환.

### 신경회로망 모델 표현 (Neural Network Model Representation)

- **단층 신경망**: 입력 층과 출력 층만으로 구성된 간단한 구조.
- **다층 신경망**: 중간에 하나 이상의 은닉 층이 추가되어 복잡한 패턴을 학습할 수 있는 구조.
- **순전파와 역전파**: 순전파는 입력 값을 통해 출력을 계산하는 과정이며, 역전파는 출력 값의 오차를 이용하여 가중치를 조정하는 과정.

### 논리 함수 구현 (Implementation of Logical Functions)

- **AND, OR, XOR** 등의 논리 함수를 신경망을 통해 구현할 수 있다.
- 다층 퍼셉트론을 사용하면 XOR와 같은 선형적으로 분리할 수 없는 문제도 해결 가능.

### 실제 사례

1. **유튜브 비디오 클러스터링**
   - 유튜브에 업로드된 비디오를 자동으로 분류하고, 가장 적합한 인덱스를 할당.
   - 비디오 콘텐츠의 유사성을 기반으로 클러스터를 형성.

2. **DNA 클러스터링**
   - 특정 유전자를 얼마나 많이 가지고 있는지를 측정하여 개체를 그룹화.
   - 유전 정보의 유사성을 바탕으로 클러스터를 형성.

이와 같은 내용을 통해 신경회로망 모델링의 기본 개념과 실제 응용 사례를 이해할 수 있습니다.

## 수학 복습 - 벡터와 행렬 (Review of Mathematics: Vectors and Matrices)

### 학습 목표

1. 벡터와 행렬의 개념을 설명할 수 있다.

### 벡터란 무엇인가? (What is a Vector?)

- 벡터는 숫자들의 단일 배열로 구성된다.
- 예시: 4차원 벡터
  - 21
  - 65
  - 34
  - 77

### 벡터의 표현

- **Column 벡터**: 세로로 나열된 숫자들의 배열.
- **Row 벡터**: 가로로 나열된 숫자들의 배열.
- **Transpose (전치)**: Row 벡터를 Column 벡터로 또는 Column 벡터를 Row 벡터로 변환.

### 행렬이란 무엇인가? (What is a Matrix?)

- 행렬은 숫자들이 직사각형 형태로 배열된 2차원 배열이다.
- 행렬은 벡터를 확장한 개념으로, 여러 벡터를 하나의 구조로 결합한 형태이다.

### 행렬의 연산

- **덧셈**: 같은 크기의 두 행렬의 각 원소를 더한다.
- **뺄셈**: 같은 크기의 두 행렬의 각 원소를 뺀다.
- **곱셈**: 행렬의 곱셈은 일반적으로 행렬 A의 행과 행렬 B의 열을 곱하여 합산한다.
- **전치 행렬**: 행렬의 행과 열을 교환한 새로운 행렬.

### 벡터와 행렬의 응용

- **벡터 내적**: 두 벡터의 내적은 두 벡터의 크기와 방향의 유사성을 측정한다.
- **벡터 노름**: 벡터의 크기를 나타내는 척도로, 유클리드 노름이 대표적이다.
- **행렬 연산의 응용**: 데이터 변환, 회전, 스케일링 등 다양한 수학적 변환에 사용된다.

이와 같은 내용을 통해 벡터와 행렬의 기본 개념과 연산 방법을 이해할 수 있습니다.

## 수학 복습 - 내적, 노름, 전치 행렬 및 행렬식 (Review of Mathematics: Inner Product, Norm, Transpose, and Determinant)

### 내적 (Inner Product)

- **내적 (Scalar Product)**: 두 벡터의 각 원소를 곱한 후, 그 결과를 더한 값.
  - 예시: 벡터 $( x $)와 벡터 $( y $)의 내적은 다음과 같다.
    $[
    x $cdot y = x_1 y_1 + x_2 y_2 + $cdots + x_n y_n
    $]
- **코사인 각도 (Cosine Angle)**: 두 벡터 사이의 각도를 나타내며, 내적을 이용하여 계산할 수 있다.
  $[
  $cos($theta) = $frac{x $cdot y}{$|x$| $|y$|}
  $]

### 벡터 노름 (Vector Norm)

- **유클리드 노름 (Euclidean Norm)**: 벡터의 길이를 측정하는 방법으로, 벡터의 각 원소의 제곱합의 제곱근으로 계산된다.
  $[
  $|x$|_2 = $sqrt{x_1^2 + x_2^2 + $cdots + x_n^2}
  $]

### 전치 행렬 (Matrix Transpose)

- 행과 열을 맞바꾼 행렬.
  - 예시: 행렬 $( A $)의 전치 행렬 $( A^T $)는 다음과 같다.
    $[
    A = $begin{bmatrix}
    1 & 2 $$
    3 & 4
    $end{bmatrix}
    $Rightarrow
    A^T = $begin{bmatrix}
    1 & 3 $$
    2 & 4
    $end{bmatrix}
    $]

### 대칭 행렬 (Symmetric Matrix)

- 대칭을 이루는 행렬은 자기 자신의 전치 행렬과 같다.
  - 예시: 행렬 $( A $)가 대칭 행렬인 경우,
    $[
    A = A^T
    $]
  - 예시: 대칭 행렬
    $[
    $begin{bmatrix}
    1 & 2 $$
    2 & 3
    $end{bmatrix}
    $]

### 행렬식 (Determinant)

- 행렬의 크기를 측정하는 값.
  - 예시: 2x2 행렬 $( A $)의 행렬식 $( $text{det}(A) $)는 다음과 같다.
    $[
    A = $begin{bmatrix}
    1 & 2 $$
    3 & 4
    $end{bmatrix}
    $Rightarrow
    $text{det}(A) = (1 $cdot 4) - (2 $cdot 3) = 4 - 6 = -2
    $]

## 수학 복습 - 행렬 연산 (Review of Mathematics: Matrix Operations)

## 수학 복습 - 단위 행렬과 역행렬 (Review of Mathematics: Identity Matrix and Inverse Matrix)

### 단위 행렬 (Identity Matrix)

- 단위 행렬은 주대각선의 원소가 모두 1이고 나머지 원소가 모두 0인 행렬이다.
- 단위 행렬 $I$와 행렬 $A$의 곱셈은 행렬 $A$를 변화시키지 않는다.
  - 예시:
    \[
    I = \begin{bmatrix}
    1 & 0 \\
    0 & 1
    \end{bmatrix}
    \]
    \[
    A = \begin{bmatrix}
    3 & 4 \\
    2 & 1
    \end{bmatrix}
    \]
    \[
    A \cdot I = \begin{bmatrix}
    3 & 4 \\
    2 & 1
    \end{bmatrix}
    \]

### 역행렬 (Matrix Inverse)

- 역행렬은 주어진 행렬 $A$에 대해 $A \cdot A^{-1} = I$를 만족하는 행렬 $A^{-1}$를 의미한다.
- 역행렬은 오직 정방행렬(행과 열의 수가 같은 행렬)에서만 존재할 수 있다.
  - 예시:
    \[
    A = \begin{bmatrix}
    1 & 2 \\
    3 & 4
    \end{bmatrix}
    \]
    \[
    A^{-1} = \begin{bmatrix}
    -2 & 1 \\
    1.5 & -0.5
    \end{bmatrix}
    \]
    \[
    A \cdot A^{-1} = \begin{bmatrix}
    1 & 0 \\
    0 & 1
    \end{bmatrix}
    \]

### 역행렬의 존재 조건

- 행렬의 행렬식(determinant)이 0이 아니어야 한다.
- 행렬식이 0인 행렬은 역행렬이 존재하지 않는 비가역 행렬(non-invertible matrix)이다.
  - 예시:
    \[
    A = \begin{bmatrix}
    1 & 2 \\
    2 & 4
    \end{bmatrix}
    \]
    - 이 행렬의 행렬식은 $1 \cdot 4 - 2 \cdot 2 = 0$이므로, 역행렬이 존재하지 않는다.

### 역행렬 계산 방법

1. **공식 사용**: 주어진 행렬이 $2 \times 2$ 행렬일 경우, 다음 공식을 사용하여 역행렬을 계산할 수 있다.
   \[
   A = \begin{bmatrix}
   a & b \\
   c & d
   \end{bmatrix}
   \Rightarrow
   A^{-1} = \frac{1}{ad - bc} \begin{bmatrix}
   d & -b \\
   -c & a
   \end{bmatrix}
   \]

2. **가우스-조던 소거법**: 행렬을 단위 행렬로 변환하는 과정을 통해 역행렬을 계산하는 방법.

이와 같은 내용을 통해 단위 행렬과 역행렬의 기본 개념과 계산 방법을 이해할 수 있습니다.

## 선형 회귀 - 모델 표현 (Linear Regression: Model Representation)

### 학습 목표

1. 선형 회귀 모델을 수학적으로 표현할 수 있다.

### 자동차 판매 가격 예측 문제 (Automobile Sale Price Prediction Problem)

#### 데이터

- 엔진 출력 (Engine Power, hp)
- 자동차 가격 (Car Price, $)

#### 질문

- 115마력의 자동차 가격은 얼마인가?

### 선형 회귀 모델 (Linear Regression Model)

- 선형 회귀 함수는 엔진 출력을 기반으로 자동차 가격을 예측하는 함수이다.
- 지도 학습 (Supervised Learning): 데이터의 각 예제에 대해 "정답"이 주어진다.

### 예측 (Prediction)

- 115마력의 엔진 출력을 가진 자동차의 가격을 예측하기 위해 회귀 모델을 사용한다.
- 예시: 115마력의 엔진 출력을 가진 자동차는 약 2,300달러의 가격을 가질 것으로 예측된다.

### 단변량 선형 회귀 (Univariate Linear Regression)

- 입력 값 (x)에 대해 목표 값 (y)을 예측하는 문제.
- 가설 (Hypothesis): \( h \)
- 학습 알고리즘 (Learning Algorithm)
- 훈련 데이터 세트 (Training Data Set)

### 가설 함수 (Hypothesis Function)

- 가설 함수 \( h \)는 입력 값 \( x \)를 출력 값 \( y \)로 매핑하는 함수이다.
  \[
  h(x) = \theta_0 + \theta_1 x
  \]

### 비용 함수 (Cost Function)

- 비용 함수는 가설 함수의 예측 값과 실제 값 간의 차이를 측정한다.
  \[
  J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)})^2
  \]

### 경사 하강법 (Gradient Descent)

- 비용 함수를 최소화하기 위해 경사 하강법을 사용한다.
  \[
  \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta)
  \]
  - 여기서 \( \alpha \)는 학습률(learning rate)을 의미한다.

## 선형 회귀 - 비용 함수 (Linear Regression: Cost Function)

### 문제 정의 (Problem Statement)

- **훈련 데이터 세트 (Training Data Set)**
  - 데이터 샘플 페어 \((x^{(i)}, y^{(i)})\)
- **가설 (Hypothesis)**
  \[
  h(x) = w_0 + w_1 x
  \]
- **파라미터 결정 (Determine Parameters \( w_0 \) and \( w_1 \))**

### 비용 함수 (Cost Function)

- 비용 함수는 가설 함수의 예측 값과 실제 값 간의 차이를 측정하는 함수이다.
- 비용 함수 \( J(\theta) \)의 목적은 파라미터를 조정하여 예측 오차를 최소화하는 것이다.
  \[
  J(w_0, w_1) = \frac{1}{2m} \sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)})^2
  \]

### 경사 하강법 (Gradient Descent)

- 비용 함수를 최소화하기 위해 경사 하강법을 사용한다.
- 경사 하강법 알고리즘:
  \[
  w_j := w_j - \alpha \frac{\partial}{\partial w_j} J(w_0, w_1)
  \]
  - 여기서 \( \alpha \)는 학습률(learning rate)을 의미한다.
- 매 반복마다 비용 함수 \( J(w_0, w_1) \)를 최소화하는 방향으로 파라미터 \( w_0 \)와 \( w_1 \)을 업데이트 한다.

### 선형 회귀 모델의 수학적 표현

1. **가설 함수 (Hypothesis Function)**
   - 가설 함수 \( h(x) \)는 다음과 같이 표현된다:
     \[
     h(x) = w_0 + w_1 x
     \]

2. **비용 함수 (Cost Function)**
   - 비용 함수는 가설 함수의 예측 값과 실제 값 간의 차이를 제곱하여 평균을 구한 값이다:
     \[
     J(w_0, w_1) = \frac{1}{2m} \sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)})^2
     \]

3. **경사 하강법 (Gradient Descent)**
   - 비용 함수를 최소화하기 위해 파라미터를 업데이트 하는 알고리즘:
     \[
     w_j := w_j - \alpha \frac{\partial}{\partial w_j} J(w_0, w_1)
     \]

## 비용 함수와 등고선 시각화 (Cost Function and Contour Visualization)

### 등고선 해석 (Contour Interpretation)

- 비용 함수의 등고선은 동일한 비용 함수 값을 가지는 선들을 나타낸다.
- 등고선을 통해 비용 함수의 최적화 경로와 파라미터 변화에 따른 비용 함수 값을 시각적으로 이해할 수 있다.

### 비용 함수 등고선 시각화 (Contour Visualization)

- 비용 함수의 등고선 그래프는 비용 함수 값을 시각적으로 표현한다.
- 등고선 그래프 예시:
  \[
  J(w_0, w_1)
  \]
  \[
  \text{등고선} (w_0, w_1)
  \]

### 자동차 데이터 세트 (Automobile Data Set)

- 자동차 데이터 세트를 사용하여 비용 함수의 등고선 그래프를 그린다.
- 예측 함수:
  \[
  h(x) = 6781.0 - 175.68x
  \]

- 또 다른 예측 함수:
  \[
  h(x) = 2303.5 - 128.38x
  \]

### 목적 (Objective)

- 비용 함수의 등고선 그래프를 통해 최적의 파라미터 \( w_0 \)와 \( w_1 \)를 찾는 과정 이해.
- 비용 함수 값이 최소화되는 지점을 찾기 위한 시각적 도구로 활용.

### 실제 적용 (Practical Application)

- 등고선 그래프는 경사 하강법을 사용하여 비용 함수 값을 최소화하는 최적의 파라미터를 찾는 데 유용하다.
- 비용 함수의 최저점을 찾는 과정에서 파라미터의 변화에 따른 비용 함수 값을 시각적으로 분석할 수 있다.

이와 같은 내용을 통해 비용 함수와 등고선 시각화의 기본 개념과 최적화 과정에서의 응용 방법을 이해할 수 있습니다.
